# -*- coding: utf-8 -*-
"""analise_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxuUd7qGAUkGpHUSufPOe1tGgjv_EF9I
"""

import pandas as pd

# Load the uploaded file to understand its structure
file_path = 'Redacoes_comeco.xlsx'
data = pd.ExcelFile(file_path)

# Display sheet names to decide the relevant sheet for EDA
data.sheet_names

# Instalar o modelo do SpaCy para português
!python -m spacy download pt_core_news_sm
!python -m spacy download pt_core_news_lg
!pip install enelvo sentence_transformers
!pip install spacy-lookups-data

# Load the relevant sheet to explore its structure
df = data.parse('Redacoes_comeco')

# Display the first few rows of the data to understand its structure
df.head()

# Plotar a distribuição das notas
plt.figure(figsize=(10, 6))
plt.hist(df['Nota'], bins=10, edgecolor='black', alpha=0.7, color='skyblue')
plt.title('Distribuição das Notas', fontsize=16)
plt.xlabel('Nota', fontsize=14)
plt.ylabel('Frequência', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import pandas as pd
import spacy

# Carregar o modelo SpaCy em português
nlp = spacy.load("pt_core_news_sm")

# Carregar o arquivo Excel
file_path = 'Redacoes_comeco.xlsx'  # Substitua pelo caminho correto do arquivo
data = pd.ExcelFile(file_path)

# Carregar a planilha relevante
df = data.parse('Redacoes_comeco')

# Função para extrair entidades de uma redação
def extract_entities(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# Aplicar a extração de entidades em cada redação
df['Entidades'] = df['Redação'].apply(lambda x: extract_entities(x))

# Contar o número de entidades por redação
df['Quantidade de Entidades'] = df['Entidades'].apply(len)

# Separar os tipos de entidades
df['Tipos de Entidades'] = df['Entidades'].apply(lambda ents: [ent[1] for ent in ents])

# Analisar a relação entre quantidade de entidades e nota
relation = df[['Nota', 'Quantidade de Entidades']].groupby('Nota').mean()

# Exibir o DataFrame atualizado
print(df[['Nota', 'Quantidade de Entidades', 'Tipos de Entidades']].head())
print("\nMédia de entidades por nota:")
print(relation)

# Visualizar a relação entre nota e quantidade de entidades
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(df['Nota'], df['Quantidade de Entidades'], alpha=0.7, color='blue')
plt.title('Relação entre Nota e Quantidade de Entidades', fontsize=16)
plt.xlabel('Nota', fontsize=14)
plt.ylabel('Quantidade de Entidades', fontsize=14)
plt.grid(alpha=0.4)
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModel
from sklearn.feature_extraction.text import CountVectorizer
import torch
import numpy as np
from collections import Counter
import spacy

# Carregar modelo SpaCy para português
nlp = spacy.load("pt_core_news_sm")

# Carregar o arquivo Excel
file_path = 'Redacoes_comeco.xlsx'  # Substitua pelo caminho correto do arquivo
data = pd.ExcelFile(file_path)
df = data.parse('Redacoes_comeco')

# Função para remover stop words e preprocessar texto
def preprocess_text(text):
    doc = nlp(text)
    return " ".join([token.text for token in doc if not token.is_stop and token.is_alpha])

df['Processed_Text'] = df['Redação'].apply(preprocess_text)

# Carregar modelo BERT para embeddings
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
model = AutoModel.from_pretrained("bert-base-multilingual-cased")

# Função para gerar embeddings com BERT
def generate_embeddings(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()
    return embeddings.numpy()

# Gerar embeddings para cada redação
df['Embedding'] = df['Processed_Text'].apply(generate_embeddings)

# Converter embeddings para matriz numpy
embedding_matrix = np.stack(df['Embedding'].values)

# Clusterização usando K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
df['Cluster'] = kmeans.fit_predict(embedding_matrix)

# Análise do conteúdo de cada cluster
def analyze_cluster(cluster_id):
    cluster_texts = df[df['Cluster'] == cluster_id]['Processed_Text']
    vectorizer = CountVectorizer(max_features=20)
    X = vectorizer.fit_transform(cluster_texts)
    word_counts = np.asarray(X.sum(axis=0)).flatten()
    words = vectorizer.get_feature_names_out()
    return dict(zip(words, word_counts))

cluster_0_analysis = analyze_cluster(0)
cluster_1_analysis = analyze_cluster(1)

# Redução de dimensionalidade para visualização (t-SNE)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
reduced_embeddings = tsne.fit_transform(embedding_matrix)

# Adicionar as coordenadas reduzidas ao DataFrame
df['TSNE_1'] = reduced_embeddings[:, 0]
df['TSNE_2'] = reduced_embeddings[:, 1]

# Plotar os clusters
plt.figure(figsize=(10, 6))
plt.scatter(df['TSNE_1'], df['TSNE_2'], c=df['Cluster'], cmap='viridis', alpha=0.7)
plt.colorbar(label='Cluster')
plt.title('Clusterização das Redações com Embeddings de BERT', fontsize=16)
plt.xlabel('TSNE Dimensão 1', fontsize=14)
plt.ylabel('TSNE Dimensão 2', fontsize=14)
plt.grid(alpha=0.4)
plt.show()

# Exibir palavras mais comuns por cluster
print("Palavras mais comuns no Cluster 0:")
print(cluster_0_analysis)

print("\nPalavras mais comuns no Cluster 1:")
print(cluster_1_analysis)

# Analisar o conteúdo dos clusters
cluster_analysis = df.groupby('Cluster')['Nota'].mean()
print("\nMédia de notas por cluster:")
print(cluster_analysis)

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# Carregar o arquivo Excel
file_path = 'Redacoes_comeco.xlsx'  # Substitua pelo caminho correto do arquivo
data = pd.ExcelFile(file_path)
df = data.parse('Redacoes_comeco')

# Carregar modelo Sentence-BERT
model = SentenceTransformer('all-MiniLM-L6-v2')  # Modelo otimizado para análise semântica

# Geração de embeddings para cada redação
df['Embedding'] = df['Redação'].apply(lambda x: model.encode(x, convert_to_numpy=True))

# Converter embeddings para matriz numpy
embedding_matrix = np.stack(df['Embedding'].values)

# Clusterização usando K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
df['Cluster'] = kmeans.fit_predict(embedding_matrix)

# Matriz de similaridade semântica (cosine similarity)
similarity_matrix = cosine_similarity(embedding_matrix)

# Análise de clusters: Média das notas por cluster
cluster_analysis = df.groupby('Cluster')['Nota'].mean()

# Visualização da similaridade com t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
reduced_embeddings = tsne.fit_transform(embedding_matrix)

# Adicionar coordenadas reduzidas ao DataFrame
df['TSNE_1'] = reduced_embeddings[:, 0]
df['TSNE_2'] = reduced_embeddings[:, 1]

# Plotar os clusters semânticos
plt.figure(figsize=(10, 6))
plt.scatter(df['TSNE_1'], df['TSNE_2'], c=df['Cluster'], cmap='viridis', alpha=0.7)
plt.colorbar(label='Cluster')
plt.title('Clusters Semânticos das Redações (Sentence-BERT)', fontsize=16)
plt.xlabel('TSNE Dimensão 1', fontsize=14)
plt.ylabel('TSNE Dimensão 2', fontsize=14)
plt.grid(alpha=0.4)
plt.show()

# Exibir resultados
print("\nMédia de notas por cluster:")
print(cluster_analysis)

# Análise de redações mais próximas semanticamente
most_similar_indices = np.unravel_index(np.argmax(similarity_matrix, axis=None), similarity_matrix.shape)
print("\nRedações mais similares semanticamente:")
print(f"Redação 1:\n{df.iloc[most_similar_indices[0]]['Redação']}")
print(f"Redação 2:\n{df.iloc[most_similar_indices[1]]['Redação']}")

# Análise de redações mais distantes semanticamente
least_similar_indices = np.unravel_index(np.argmin(similarity_matrix, axis=None), similarity_matrix.shape)
print("\nRedações menos similares semanticamente:")
print(f"Redação 1:\n{df.iloc[least_similar_indices[0]]['Redação']}")
print(f"Redação 2:\n{df.iloc[least_similar_indices[1]]['Redação']}")

import spacy
spacy.cli.download("en_core_web_lg")

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
import spacy
from spacy.lang.pt.stop_words import STOP_WORDS
import string
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from scipy.stats import spearmanr
from tqdm import tqdm

# Certifique-se de que SpaCy está utilizando a GPU (se disponível)
spacy.require_gpu()
nlp = spacy.load("pt_core_news_sm")

# Lista de stopwords e pontuações
stopwords_pt = set(STOP_WORDS)
punctuation = set(string.punctuation)

# Carregar o arquivo Excel com as redações
file_path = 'Redacoes_comeco.xlsx'  # Substitua pelo caminho correto do arquivo
df = pd.ExcelFile(file_path).parse('Redacoes_comeco')

# Remover linhas nulas e garantir que o texto esteja limpo
df.dropna(subset=['Redação'], inplace=True)

# Pré-processar textos
def preprocess_texts(texts, batch_size=256):
    processed_texts = []
    for docs in tqdm(nlp.pipe(texts, batch_size=batch_size), desc="Pré-processando textos", total=len(texts)):
        tokens = [
            token.lemma_ for token in docs
            if token.is_alpha and token.lemma_ not in stopwords_pt and token.lemma_ not in punctuation
        ]
        processed_texts.append(' '.join(tokens))
    return processed_texts

df['Processed_Text'] = preprocess_texts(df['Redação'])

# Criar TF-IDF para as redações
vectorizer = TfidfVectorizer(max_features=1000, max_df=0.8, min_df=5, sublinear_tf=True)
tfidf_matrix = vectorizer.fit_transform(df['Processed_Text'])

# Calcular correlações entre palavras e notas
feature_names = vectorizer.get_feature_names_out()
correlations = {}
for i, word in enumerate(feature_names):
    correlations[word] = spearmanr(tfidf_matrix[:, i].toarray().flatten(), df['Nota']).correlation

# Ordenar palavras por correlação
sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)

# Exibir as palavras mais correlacionadas
print("\nPalavras mais correlacionadas com as notas:")
for word, corr in sorted_correlations[:20]:
    print(f"{word}: {corr:.3f}")

# Visualizar as palavras mais correlacionadas
plt.figure(figsize=(12, 6))
top_words = [word for word, _ in sorted_correlations[:20]]
top_corrs = [corr for _, corr in sorted_correlations[:20]]
plt.barh(top_words, top_corrs, color='skyblue')
plt.xlabel("Correlação", fontsize=14)
plt.title("Palavras mais correlacionadas com as Notas", fontsize=16)
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Análise exploratória: Word Cloud para as redações de alta e baixa nota
def generate_wordcloud(reviews, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(reviews))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

# Separar redações de alta e baixa nota
high_note_reviews = df[df['Nota'] >= 900]['Processed_Text']
low_note_reviews = df[df['Nota'] < 900]['Processed_Text']

# Gerar Word Cloud para ambas
generate_wordcloud(high_note_reviews, "Word Cloud - Redações de Alta Nota")
generate_wordcloud(low_note_reviews, "Word Cloud - Redações de Baixa Nota")

import pandas as pd
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from scipy.spatial.distance import cosine
from scipy.stats import spearmanr
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter

# Configuração do SpaCy e modelo para embeddings
spacy.require_gpu()
nlp = spacy.load("pt_core_news_sm")
sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Carregar o arquivo Excel
file_path = 'Redacoes_comeco.xlsx'  # Substitua pelo caminho correto
df = pd.ExcelFile(file_path).parse('Redacoes_comeco')

# Remover redações vazias
df = df.dropna(subset=['Redação'])

# Função para calcular métricas avançadas
def analyze_text(text):
    doc = nlp(text)
    metrics = {}

    # Coesão: Contagem de conectivos
    connectives = ["portanto", "além disso", "porém", "assim", "contudo", "entretanto"]
    metrics['Coesão'] = sum([text.lower().count(word) for word in connectives])

    # Coerência: Similaridade entre frases consecutivas
    sentences = [sent.text for sent in doc.sents]
    sentence_embeddings = [sentence_model.encode(sent) for sent in sentences]
    if len(sentence_embeddings) > 1:
        metrics['Coerência'] = np.mean(
            [1 - cosine(sentence_embeddings[i], sentence_embeddings[i + 1]) for i in range(len(sentence_embeddings) - 1)]
        )
    else:
        metrics['Coerência'] = 0

    # Complexidade lexical
    words = [token.text.lower() for token in doc if token.is_alpha]
    unique_words = set(words)
    metrics['Diversidade Lexical'] = len(unique_words) / len(words) if words else 0

    # Densidade lexical: Substantivos, verbos e adjetivos
    metrics['Densidade Lexical'] = sum(1 for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ']) / len(words) if words else 0

    # Número de entidades nomeadas
    metrics['Entidades Nomeadas'] = len(doc.ents)

    return metrics

# Aplicar análise às redações
metrics_list = df['Redação'].apply(analyze_text)

# Criar um DataFrame com as métricas
metrics_df = pd.DataFrame(metrics_list.tolist())
metrics_df['Nota'] = df['Nota']

# Calcular correlações entre métricas e notas
correlations = {col: spearmanr(metrics_df[col], metrics_df['Nota']).correlation for col in metrics_df.columns if col != 'Nota'}

# Exibir correlações
correlation_df = pd.DataFrame(list(correlations.items()), columns=['Métrica', 'Correlação']).sort_values(by='Correlação', ascending=False)
print("\nCorrelação das Métricas com as Notas:")
print(correlation_df)

# Visualizar as métricas mais correlacionadas
plt.figure(figsize=(12, 6))
plt.barh(correlation_df['Métrica'], correlation_df['Correlação'], color='skyblue')
plt.xlabel("Correlação", fontsize=14)
plt.title("Correlação das Métricas com as Notas", fontsize=16)
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Análise de redações de alta e baixa nota
high_notes = metrics_df[metrics_df['Nota'] >= 900].mean()
low_notes = metrics_df[metrics_df['Nota'] < 900].mean()

print("\nMédia das Métricas para Redações de Alta Nota:")
print(high_notes)

print("\nMédia das Métricas para Redações de Baixa Nota:")
print(low_notes)

import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
from tqdm import tqdm
from scipy.stats import spearmanr

# Configuração do SpaCy
spacy.require_gpu()
nlp = spacy.load("pt_core_news_sm")

# Carregar o arquivo Excel
file_path = 'Redacoes_comeco.xlsx'  # Substitua pelo caminho correto
df = pd.ExcelFile(file_path).parse('Redacoes_comeco')

# Remover redações vazias
df = df.dropna(subset=['Redação'])

# Função para identificar categorias gramaticais
def analyze_pos(text):
    doc = nlp(text)
    pos_counts = Counter(token.pos_ for token in doc)
    total_words = sum(pos_counts.values())
    return {
        'Substantivos': pos_counts.get('NOUN', 0),
        'Verbos': pos_counts.get('VERB', 0),
        'Adjetivos': pos_counts.get('ADJ', 0),
        'Advérbios': pos_counts.get('ADV', 0),
        'Total Palavras': total_words,
        'Proporção Substantivos': pos_counts.get('NOUN', 0) / total_words if total_words > 0 else 0,
        'Proporção Verbos': pos_counts.get('VERB', 0) / total_words if total_words > 0 else 0,
        'Proporção Adjetivos': pos_counts.get('ADJ', 0) / total_words if total_words > 0 else 0,
        'Proporção Advérbios': pos_counts.get('ADV', 0) / total_words if total_words > 0 else 0,
    }

# Aplicar a análise em todas as redações
tqdm.pandas(desc="Processando redações")
pos_metrics = df['Redação'].progress_apply(analyze_pos)

# Converter resultados em DataFrame
pos_df = pd.DataFrame(pos_metrics.tolist())
pos_df['Nota'] = df['Nota']

# Correlação entre métricas e notas
correlations = {
    col: spearmanr(pos_df[col], pos_df['Nota']).correlation
    for col in pos_df.columns if col != 'Nota'
}

# Exibir as correlações
correlation_df = pd.DataFrame(list(correlations.items()), columns=['Métrica', 'Correlação']).sort_values(by='Correlação', ascending=False)
print("\nCorrelação das Métricas com as Notas:")
print(correlation_df)

# Visualização das proporções
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
categories = ['Proporção Substantivos', 'Proporção Verbos', 'Proporção Adjetivos', 'Proporção Advérbios']
for ax, category in zip(axes.flat, categories):
    ax.scatter(pos_df['Nota'], pos_df[category], alpha=0.6)
    ax.set_title(f"{category} vs Nota")
    ax.set_xlabel("Nota")
    ax.set_ylabel(category)
plt.tight_layout()
plt.show()

# Estatísticas descritivas para alta e baixa nota
high_notes = pos_df[pos_df['Nota'] >= 900].mean()
low_notes = pos_df[pos_df['Nota'] < 900].mean()

print("\nMédia das Métricas para Redações de Alta Nota:")
print(high_notes)

print("\nMédia das Métricas para Redações de Baixa Nota:")
print(low_notes)

import seaborn as sns

# Criar a matriz de correlação das métricas com as notas
correlation_matrix = pos_df.corr()

# Visualizar a matriz de correlação usando um heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Matriz de Correlação entre Métricas e Notas", fontsize=16)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from scipy.spatial.distance import cosine
from scipy.stats import spearmanr
import spacy

# Configuração de SpaCy e Sentence-BERT
spacy.require_gpu()
nlp = spacy.load("pt_core_news_sm")
sentence_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# Carregar o arquivo Excel com as redações
file_path = 'Redacoes_comeco.xlsx'  # Substitua pelo caminho correto
df = pd.ExcelFile(file_path).parse('Redacoes_comeco')

# Remover redações vazias
df = df.dropna(subset=['Redação'])

# Função para calcular similaridade entre frases
def analyze_sentence_similarity(text):
    doc = nlp(text)
    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 0]
    if len(sentences) < 2:  # Precisa de pelo menos 2 frases para calcular similaridade
        return {
            'Similaridade Média': 0,
            'Similaridade Mínima': 0,
            'Similaridade Máxima': 0,
        }

    # Gerar embeddings para as frases
    embeddings = sentence_model.encode(sentences)

    # Calcular similaridades de cosseno entre todas as combinações de frases
    similarities = [
        1 - cosine(embeddings[i], embeddings[j])
        for i in range(len(embeddings)) for j in range(i + 1, len(embeddings))
    ]

    return {
        'Similaridade Média': np.mean(similarities),
        'Similaridade Mínima': np.min(similarities),
        'Similaridade Máxima': np.max(similarities),
    }

# Aplicar a análise de similaridade em todas as redações
tqdm.pandas(desc="Analisando similaridade entre frases")
similarity_metrics = df['Redação'].progress_apply(analyze_sentence_similarity)

# Converter resultados em DataFrame
similarity_df = pd.DataFrame(similarity_metrics.tolist())
similarity_df['Nota'] = df['Nota']

# Correlação entre métricas de similaridade e notas
correlations = {
    col: spearmanr(similarity_df[col], similarity_df['Nota']).correlation
    for col in similarity_df.columns if col != 'Nota'
}

# Exibir as correlações
correlation_df = pd.DataFrame(list(correlations.items()), columns=['Métrica', 'Correlação']).sort_values(by='Correlação', ascending=False)
print("\nCorrelação das Métricas de Similaridade com as Notas:")
print(correlation_df)

# Visualizar correlações com heatmap
plt.figure(figsize=(8, 5))
sns.heatmap(similarity_df.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Matriz de Correlação - Similaridade de Frases e Notas", fontsize=14)
plt.tight_layout()
plt.show()

# Visualização da relação entre similaridade média e nota
plt.figure(figsize=(10, 6))
plt.scatter(similarity_df['Nota'], similarity_df['Similaridade Média'], alpha=0.6, color='blue')
plt.title("Similaridade Média das Frases vs Nota", fontsize=16)
plt.xlabel("Nota", fontsize=14)
plt.ylabel("Similaridade Média", fontsize=14)
plt.grid(alpha=0.4)
plt.tight_layout()
plt.show()